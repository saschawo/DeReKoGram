{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing DeReKoGram\n",
    "======================\n",
    "\n",
    "Alexander Koplenig\n",
    "\n",
    "contact: <koplenig@ids-mannheim.de>\n",
    "\n",
    "2023-03-06\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is this document?\n",
    "\n",
    "This document accompanies a short paper [[1](#1-wolfer-sascha-koplenig-alexander-kupietz-marc-müller-spitzer-carolin-submitted-introducing-derekogram)]. With this document, we want to demonstrate some basic use cases for the 1- to 3-gram frequency lists provided in the repository of the Leibniz Institute for the German Language (IDS) and make it easier for other researchers to work with the data. \n",
    "In this document we are providing **Stata** code (```version 17```) to work with the frequency lists.\n",
    "##How to use?\n",
    "\n",
    "* First, you need to download the dataset(s) you want to work with in the \n",
    "[IDS repository](https://hdl.handle.net/10932/00-057D-0921-30F0-F201-D)\n",
    ". The naming convention of the datasets is as follows: ```[1/2/3]-gram-token-lemma-pos-freqs-[with/without]-punctuation.<fold number with leading zero>.tsv.```\n",
    "* Then unzip the dataset(s) with ```unxz``` on Linux or macOS. On Windows, you can use \n",
    "[7-Zip](https://www.7-zip.org/download.html) \n",
    "to decompress xz files.\n",
    "\n",
    "We will demonstrate most of the operations documented here with only a subset of the 16 folds contained in the original data. Please refer to the accompanying publication (Section ‘Evaluation of fold distribution’) to see why, in many cases, it should not make a huge difference whether you use a subset of folds or all folds.\n",
    "\n",
    "All following sections are ‘self-contained’ in the sense that no code in these sections relies on code in previous sections (e.g. loading of data).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the environment and set the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/koplenig/DeReKoGram'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "os.chdir('/home/koplenig/DeReKoGram')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decrypting’ integer codes\n",
    "\n",
    "Wordforms and lemmas in DeReKoGram are integer-coded  [[2](#2-brants-thorsten-popat-ashok-c-xu-peng-och-franz-j-dean-jeffrey-2007-large-language-models-in-machine-translation-in-proceedings-of-the-2007-joint-conference-on-empirical-methods-in-natural-language-processing-and-computational-natural-language-learning-emnlp-conll-858867-prague-czech-republic-association-for-computational-linguistics-retrieved-from-httpsaclanthologyorgd07-1090)]. The information for back-translating or ‘decrypting’ the integer codes is saved in the ```[lemma/token]_keys.<fold number with leading zero>.tsv.xz``` dictionary files. The complete mapping over all folds can be found in ```[lemma/token]_keys.all.tsv.xz```. The files without fold number are the overall dictionaries for the complete dataset. Integer codes are consistent over the single-fold dictionaries and the overall dictionary.\n",
    "\n",
    "As we wrote in the article, the columns in the unigram datasets are sorted as follows\n",
    "1: ```Wordform code``` TAB 2: ```Lemma code``` TAB 3: ```POS tag``` TAB 4: ```Frequency```\n",
    "\n",
    "The first three columns are repeated for bigrams. In this case, the first three columns are repeated for the second element of the bigram, like so:\n",
    "1: ```Wordform code 1``` TAB 2: ```Lemma code 1``` TAB 3: ```POS tag 1``` TAB 4: ```Wordform code 2``` TAB 5: ```Lemma code 2``` TAB 6: ```POS tag 2``` TAB 7: ```Frequency```\n",
    "\n",
    "For trigrams, this works accordingly. Consequently, trigram datasets have ```3 * 3 + 1 = 10 columns```.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the ten most frequent forms (integer code 0-9) with corresponding lemma & wordform information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create we create a dictionary of lemma strings and ids using \n",
    "lemma_dict = defaultdict(str)\n",
    "with open('lemma_keys.all.tsv', 'r') as lemma_file:\n",
    "    lemma_reader = csv.reader(lemma_file, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in lemma_reader:\n",
    "        lemma_string, lemma_id = row\n",
    "        lemma_dict[lemma_id] = lemma_string\n",
    "\n",
    "# Next, create a dictionary of token strings and ids\n",
    "token_dict = defaultdict(str)\n",
    "with open('token_keys.all.tsv', 'r') as token_file:\n",
    "    token_reader = csv.reader(token_file, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in token_reader:\n",
    "        token_string, token_id = row\n",
    "        token_dict[token_id] = token_string\n",
    "\n",
    "# Next, create a list of (token_string, lemma_string, pos, frequency) tuples\n",
    "fold01 = []\n",
    "with open('1-gram-token-lemma-pos-freqs-with-punctuation.01.tsv', 'r') as freq_file:\n",
    "    freq_reader = csv.reader(freq_file, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in freq_reader:\n",
    "        token_id, lemma_id, pos, frequency = row\n",
    "        fold01.append((token_dict[token_id],lemma_dict[lemma_id], pos, int(frequency)))\n",
    "\n",
    "# Sort the list by frequency in descending order\n",
    "fold01.sort(key=lambda x: x[3], reverse=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', '.', '$.', 134870098),\n",
       " (',', ',', '$,', 126965247),\n",
       " ('der', 'die', 'ART', 64935117),\n",
       " ('die', 'die', 'ART', 54882541),\n",
       " ('und', 'und', 'KON', 50302414),\n",
       " ('\"', '\"', '$(', 40678679),\n",
       " ('in', 'in', 'APPR', 38193220),\n",
       " ('den', 'die', 'ART', 25292562),\n",
       " (':', ':', '$.', 21626291),\n",
       " ('mit', 'mit', 'APPR', 19625776)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold01[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check a few word forms where the lemma should differ from its associated wordform: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagte sagen VVFIN 1904814\n",
      "ging gehen VVFIN 667330\n",
      "beste gut ADJA 192421\n",
      "Linguistinnen Linguistin NN 13\n"
     ]
    }
   ],
   "source": [
    "for token, lemma, pos, freq in fold01:\n",
    "    if token == 'ging' or token == 'sagte' or token == 'Linguistinnen' or token == 'beste':\n",
    "        print(token, lemma, pos, freq)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can do this the other way around and search for all wordforms associated with a specific lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vorgemacht vormachen VVPP 3730\n",
      "vormachen vormachen VVINF 2375\n",
      "vormacht vormachen VVFIN 505\n",
      "vorzumachen vormachen VVIZU 335\n",
      "vormachte vormachen VVFIN 158\n",
      "vormachten vormachen VVFIN 80\n",
      "Vorgemacht vormachen VVPP 66\n",
      "vormachen vormachen VVFIN 49\n",
      "vormache vormachen VVFIN 46\n",
      "Vormachen vormachen VVINF 8\n",
      "vormachst vormachen VVFIN 3\n",
      "VORGEMACHT vormachen VVPP 1\n",
      "Vormachen vormachen VVFIN 1\n"
     ]
    }
   ],
   "source": [
    "for token, lemma, pos, freq in fold01:\n",
    "    if lemma == 'vormachen':\n",
    "        print(token, lemma, pos, freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating datasets\n",
    "\n",
    "For DeReKoGram, (at least) two types of aggregations might become relevant:\n",
    "\n",
    "    1. Aggregating wordform frequencies by lemma and/or POS column. This yields, for example, a dataset with POS token frequencies (one frequency value per POS tag).\n",
    "    2. Aggregating a dataset that is a combination of multiple folds (for example when compiling frequency information from four folds into one).\n",
    "\n",
    "We will go into detail below.\n",
    "\n",
    "#### Aggregation by lemma and/or part-of-speech\n",
    "\n",
    "We are demonstrating this aggregation with the unigram dataset (fold 08).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First read in fold 08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', '.', '$.', 134713792),\n",
       " (',', ',', '$,', 126822115),\n",
       " ('der', 'die', 'ART', 64888400),\n",
       " ('die', 'die', 'ART', 54818658),\n",
       " ('und', 'und', 'KON', 50236487),\n",
       " ('\"', '\"', '$(', 40623447),\n",
       " ('in', 'in', 'APPR', 38171457),\n",
       " ('den', 'die', 'ART', 25282101),\n",
       " (':', ':', '$.', 21603035),\n",
       " ('mit', 'mit', 'APPR', 19611948)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold08 = []\n",
    "with open('1-gram-token-lemma-pos-freqs-with-punctuation.08.tsv', 'r') as freq_file:\n",
    "    freq_reader = csv.reader(freq_file, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in freq_reader:\n",
    "        token_id, lemma_id, pos, frequency = row\n",
    "        fold08.append((token_dict[token_id],lemma_dict[lemma_id], pos, int(frequency)))\n",
    "# Sort the list by frequency in descending order\n",
    "fold08.sort(key=lambda x: x[3], reverse=True)\n",
    "\n",
    "fold08[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the total number of rows in fold08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20,820,567'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{:,}'.format(len(fold08))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,839,383'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{:,}'.format(len(set([x[1] for x in fold08])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2,694,880,682'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute total token frequency\n",
    "'{:,}'.format(sum([x[3] for x in fold08]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate by lemma and sort by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('die', 235387333),\n",
       " ('.', 134795026),\n",
       " (',', 126822115),\n",
       " ('--', 93049724),\n",
       " ('in', 64110264),\n",
       " ('und', 52742151),\n",
       " ('\"', 52045982),\n",
       " ('sein', 50240035),\n",
       " ('eine', 48215708),\n",
       " ('zu', 30443236)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate by lemma\n",
    "lemma_freqs = defaultdict(int)\n",
    "for token, lemma, pos, freq in fold08:\n",
    "    lemma_freqs[lemma] += freq\n",
    "\n",
    "# Sort by frequency\n",
    "lemma_freqs = sorted(lemma_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "lemma_freqs[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique values (should be same as above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,839,383'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{:,}'.format(len(lemma_freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Assert that lemma_freqs has the same length as the number of unique lemmas in fold08\n",
    "\n",
    "if len(lemma_freqs) == len(set([x[1] for x in fold08])):\n",
    "    print('True')\n",
    "else:\n",
    "    print('False')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggegation by lemma AND by POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('die', 'ART'), 214882395),\n",
       " (('.', '$.'), 134795026),\n",
       " ((',', '$,'), 126822115),\n",
       " (('--', 'NN'), 54597573),\n",
       " (('und', 'KON'), 52742151),\n",
       " (('\"', '$('), 52045982),\n",
       " (('eine', 'ART'), 46955682),\n",
       " (('in', 'APPR'), 42021000),\n",
       " (('sein', 'VAFIN'), 38290797),\n",
       " (('--', 'NE'), 25591553)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_pos_freqs = defaultdict(int)\n",
    "\n",
    "for token, lemma, pos, freq in fold08:\n",
    "    lemma_pos_freqs[(lemma, pos)] += freq\n",
    "\n",
    "lemma_pos_freqs = sorted(lemma_pos_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "lemma_pos_freqs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining and aggregating multiple folds\n",
    "We are now loading three more folds (03, 11 and 15) which we then combine with fold 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load folds\n",
    "\n",
    "fold03 = []\n",
    "with open('1-gram-token-lemma-pos-freqs-with-punctuation.03.tsv', 'r') as freq_file:\n",
    "    freq_reader = csv.reader(freq_file, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in freq_reader:\n",
    "        token_id, lemma_id, pos, frequency = row\n",
    "        fold03.append((token_dict[token_id],lemma_dict[lemma_id], pos, int(frequency)))\n",
    "\n",
    "fold11 = []\n",
    "with open('1-gram-token-lemma-pos-freqs-with-punctuation.11.tsv', 'r') as freq_file:\n",
    "    freq_reader = csv.reader(freq_file, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in freq_reader:\n",
    "        token_id, lemma_id, pos, frequency = row\n",
    "        fold11.append((token_dict[token_id],lemma_dict[lemma_id], pos, int(frequency)))\n",
    "\n",
    "fold15 = []\n",
    "with open('1-gram-token-lemma-pos-freqs-with-punctuation.15.tsv', 'r') as freq_file:\n",
    "    freq_reader = csv.reader(freq_file, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in freq_reader:\n",
    "        token_id, lemma_id, pos, frequency = row\n",
    "        fold15.append((token_dict[token_id],lemma_dict[lemma_id], pos, int(frequency)))   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all four folds into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine fold08 with fold03, fold11, and fold15 in a new list\n",
    "fold08_03_11_15 = fold08 + fold03 + fold11 + fold15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many rows there are for periods (end-of-sentence) in the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . $. 134713792\n",
      ". . $. 134745494\n",
      ". . $. 134816153\n",
      ". . $. 134982462\n"
     ]
    }
   ],
   "source": [
    "for token, lemma, pos, freq in fold08_03_11_15:\n",
    "    if pos == \"$.\" and token == '.':\n",
    "        print(token, lemma, pos, freq)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate by token, lemma and pos and sort by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('.', '.', '$.'), 539257901),\n",
       " ((',', ',', '$,'), 507716781),\n",
       " (('der', 'die', 'ART'), 259681257),\n",
       " (('die', 'die', 'ART'), 219388266),\n",
       " (('und', 'und', 'KON'), 201076566),\n",
       " (('\"', '\"', '$('), 162767675),\n",
       " (('in', 'in', 'APPR'), 152760271),\n",
       " (('den', 'die', 'ART'), 101188094),\n",
       " ((':', ':', '$.'), 86507438),\n",
       " (('mit', 'mit', 'APPR'), 78477570)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold08_03_11_15_agg = defaultdict(int)\n",
    "\n",
    "for token, lemma, pos, freq in fold08_03_11_15:\n",
    "    fold08_03_11_15_agg[(token, lemma, pos)] += freq\n",
    "\n",
    "fold08_03_11_15_agg = sorted(fold08_03_11_15_agg.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "fold08_03_11_15_agg[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowering datasets\n",
    "By \"lowering\" we mean transforming all wordforms to lower-case (e.g. *Der* and *dEr* are transformed to *der*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rennen Rennen NN 186715\n",
      "befinden befinden VVFIN 87482\n",
      "Nato Nato NE 28031\n",
      "befinden befinden VVINF 6719\n",
      "NATO NATO NE 5759\n",
      "rennen rennen VVINF 5424\n",
      "rennen rennen VVFIN 5094\n",
      "Befinden Befinden NN 2185\n",
      "Befinden befinden VVFIN 292\n",
      "RENNEN Rennen NN 110\n",
      "naTo -- NN 86\n",
      "naTo -- ADJA 26\n",
      "naTo -- VVFIN 18\n",
      "naTo -- ADJD 14\n",
      "nato -- ADJA 13\n",
      "Befinden befinden VVINF 10\n",
      "nato -- ADJD 3\n",
      "BEFINDEN Befinden NN 2\n",
      "BEFINDEN befinden VVFIN 2\n",
      "naTo Nato NE 2\n",
      "BeFinden befinden VVFIN 1\n",
      "NAto -- ADJD 1\n",
      "NaTo -- NN 1\n",
      "befiNdeN befinden VVINF 1\n"
     ]
    }
   ],
   "source": [
    "# Use the ``.lower()`` method to lower-case a string\n",
    "for token, lemma, pos, freq in fold08:\n",
    "    if token.lower() == 'befinden' or token.lower() == 'rennen' or token.lower() == 'nato':\n",
    "        print(token, lemma, pos, freq) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Veranstaltung', 517778),\n",
       " ('Entscheidung', 456259),\n",
       " ('Ausstellung', 435816),\n",
       " ('Regierung', 428677),\n",
       " ('Richtung', 422583),\n",
       " ('Führung', 391267),\n",
       " ('Entwicklung', 384038),\n",
       " ('Leistung', 383334),\n",
       " ('Zeitung', 358011),\n",
       " ('Verfügung', 355676)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List 10 most frequent lemmas starting with a capital letter that end with 'ung' and starts with in fold08\n",
    "ung_freq = defaultdict(int)\n",
    "\n",
    "for token, lemma, pos, freq in fold08:\n",
    "    if lemma.endswith('ung') and lemma[0].isupper():\n",
    "        ung_freq[lemma] += freq\n",
    "\n",
    "ung_freq = sorted(ung_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "ung_freq[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning\n",
    "In Wolfer et al. [1], we show how different cleaning 'stages' of the dataset influence vocabulary growth when taking more and more corpus folds into consideration. Here, we want to demonstrate how this cleaning can be achieved. The cleaning stages reported in the paper are as follows: \n",
    "\n",
    " \tA. No cleaning.\n",
    " \t\t\n",
    " \tB. No punctuation, names, start-end-symbols, URLs, wordforms only consisting of numbers\n",
    " \t\t\n",
    " \tC. No wordforms containing any numbers\n",
    " \t\t\n",
    " \tD. No wordforms containing upper-case letters that follow lower-case letters\n",
    " \t\t\n",
    " \tE. Only wordforms where the TreeTagger assigned a lemma\n",
    " \t\t\n",
    " \tF. Only wordforms that are themselves (or the associated lemma) on a basic lemma list of New High German standard language [5].\n",
    "\n",
    "Cleaning stages A through D are cumulative, e.g. cleaning stage D incorporates stages B and C. Stages E and F, however, both rely on stage D. Here, we only show how to create cleaning stage B based on the original dataset (A) because this already demonstrates the logic behind the process. Cleaning the dataset can be understood as an extension of [searching for specific patterns](#searching-for-patterns), because we first identify relevant codes that we want to exclude and then exclude these codes from the frequency dataset by subsetting. We are demonstrating this using fold 14.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold14 = []\n",
    "\n",
    "with open('1-gram-token-lemma-pos-freqs-with-punctuation.14.tsv', 'r') as freq_file:\n",
    "    freq_reader = csv.reader(freq_file, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in freq_reader:\n",
    "        token_id, lemma_id, pos, frequency = row\n",
    "        fold14.append((token_dict[token_id],lemma_dict[lemma_id], pos, int(frequency)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excluding punctuation, names, start-end-symbols, URLs, wordforms only consisting of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "fold14_filtered = []\n",
    "\n",
    "for token, lemma, pos, freq in fold14:\n",
    "    if pos != '$.' and pos != '$(' and pos != '$,' and pos != '$)' and pos != 'NE' and not token.isdigit() and not re.match(\"^http[s]?://\", token):\n",
    "        fold14_filtered.append((token, lemma, pos, freq))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of tokens in fold14_filtered that are not in fold14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2,418,152'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{:,}'.format(len(fold14) - len(fold14_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on START/END markers\n",
    "\n",
    "You might want to remove all rows in the dataset that consist of start or end markers only. You can do this by excluding rows that only contain the integer codes ```-1``` or ```-2``` in the wordform and lemma column(s). \n",
    "\n",
    "We’ve included these “markers-only rows” for consistency checks, for example to check whether the frequencies (in the bigram case) for ```«START»``` ```«START»``` and ```«START»``` ```<other>``` match. In the unigram datasets, the number of markers can also be used to find out the number of documents that went into a corpus fold because each ```«START»``` marks the beginning of a document and each ```«END»``` the end of a document For simple frequency tables, the “markers-only rows” are not necessary and might even distort some measures like normed/relative frequencies.\n",
    "\n",
    "However, there are situations where keeping such rows can be important. For example, when we compute  $P_{ml} =\\frac{c(v,w)}{c(v)}$ in the following section based on the ```2-gram data```, we can use the ```1-gram data``` to get the information for the denominator $c(v)$. If we exclude the markers-only rows from the ```1-gram data```, this would lead to a denominator of 0 for v equal to ```«START»``` and thus an undefined value.\n",
    "\n",
    "When we read in the ```3-gram data``` below, we exclude the markers-only rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train smoothed *n*-gram models\n",
    "\n",
    "Here, we train a smoothed n-gram models of varying order. Probabilities are smoothed with a technique called Witten-Belll smoothing [6].\n",
    "\n",
    "We start with a smoothed 1-gram model\n",
    "\n",
    "The smoothed 1-gram probability can be calculated as follows:\n",
    "\n",
    "\n",
    "$P_{sm} (w)=λ_Κ P_{ml} (w)+ (1-λ_Κ)\\frac{1}{Κ}$\n",
    "\n",
    "where $P_{ml} (w)$ is the maximum likelihood estimate of the probability of word $w$ computed as follows:\n",
    "\n",
    "$P_{ml} (w)=\\frac{C(w)}{N}$\n",
    "\n",
    "where $c(w)$ is the number of times word $w$ occurs in the corpus and $N$ is the total number of tokens in the corpus. The discounting factor $λ_Κ$ is calculated as follows:\n",
    "\n",
    "$ (1-λ_Κ) = \\frac{1}{N+K}$\n",
    "\n",
    "where $K$ is the vocabulary of the corpus. Thus, $P_{sm} (w)$ can be calculated as follows:\n",
    "\n",
    "$P_{sm} (w)=\\frac{C(w)+1}{N+K}$\n",
    "\n",
    "\n",
    "Let's compute the distribution of smoothed 1-gram probabilities for fold08."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load fold08. But we don't replace token ids with actual tokens from the token_dict, because we want to keep the token ids for the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold08_1gram_agg = defaultdict(int)\n",
    "\n",
    "with open('1-gram-token-lemma-pos-freqs-with-punctuation.08.tsv', 'r') as freq_file:\n",
    "    freq_reader = csv.reader(freq_file, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in freq_reader:\n",
    "        token_id, lemma_id, pos, frequency = row\n",
    "        # Aggregate frequencies for each token id \n",
    "        fold08_1gram_agg[token_id] += int(frequency)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use 'fold08_1gram_agg' to calculate p_1gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute lambda_k\n",
    "\n",
    "# calculate the total number of tokens in fold08\n",
    "total_tokens = sum(fold08_1gram_agg.values())\n",
    "\n",
    "# Calculate the number of unique tokens in fold08\n",
    "unique_tokens = len(fold08_1gram_agg)\n",
    "\n",
    "# Store both in a pickle file for later use\n",
    "\n",
    "with open('total_tokens_and_unique_tokens.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump((total_tokens, unique_tokens), pickle_file)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', 0.04999791489840811),\n",
       " ('1', 0.04706038224515351),\n",
       " ('2', 0.02531099742396684),\n",
       " ('3', 0.022993389063152594),\n",
       " ('4', 0.018641451302666617),\n",
       " ('5', 0.01507430264773407),\n",
       " ('6', 0.01416443305076911),\n",
       " ('7', 0.009511099386032113),\n",
       " ('8', 0.008016323373533314),\n",
       " ('9', 0.00752293529558204)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now generate the p_1gram dictionary\n",
    "p_1gram = {}\n",
    "\n",
    "for token_id, freq in fold08_1gram_agg.items():\n",
    "    p_1gram[token_id] = (freq) / (total_tokens)\n",
    "\n",
    "# Store 'p_1gram' in a pickle file for later use\n",
    "\n",
    "with open('p_sm_1gram.pickle', 'wb') as p_1gram_file:\n",
    "    pickle.dump(p_1gram, p_1gram_file)\n",
    "\n",
    "# List first 10 items in 'p_1gram'\n",
    "list(p_1gram.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import \"token_keys.08.tsv\" that contains the token ids and the actual tokens. \n",
    "\n",
    "In the first column we have the actual tokens, in the second column we have the token ids.\n",
    "We need to create a dictionary that maps token ids to actual tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('-2', '«END»'),\n",
       " ('-1', '«START»'),\n",
       " ('0', '.'),\n",
       " ('1', ','),\n",
       " ('2', 'der'),\n",
       " ('3', 'die'),\n",
       " ('4', 'und'),\n",
       " ('5', '\"'),\n",
       " ('6', 'in'),\n",
       " ('7', 'den')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_keys08 = {}\n",
    "\n",
    "with open('token_keys.08.tsv', 'r') as token_keys_file:\n",
    "    token_keys_reader = csv.reader(token_keys_file, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in token_keys_reader:\n",
    "        token, token_id = row\n",
    "        token_keys08[token_id] = token\n",
    "\n",
    "# List first 10 items in token_keys08\n",
    "list(token_keys08.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5789201 844038 9438987 52266847 9875656 3856955 10681738 45880705 2653540 19035560\n",
      "1197905 798548 2452324 5549296 49975452 97778959 891982 20045355 4112821 45861651\n",
      "5895077 24418599 5174075 113776396 33898514 222617 9974381 28245822 68823919 14319911\n",
      "17869898 11859698 36508282 3744357 3717872 27440269 3139531 16318516 18454807 33286538\n",
      "1395181 11653453 4737733 5244309 13416537 3150565 106465647 34578141 9094561 6838996\n",
      "28587208 2652935 1823515 31642671 10580183 2011761 50004445 6153838 9589400 58011328\n",
      "41772236 37785112 4200273 13388176 26969831 4301734 12040873 2112875 3617345 30353477\n",
      "20505049 7119568 90266415 75706738 6475659 107507793 4310080 3656514 1521044 12920044\n",
      "1212325 18243134 27841156 9944781 1803334 3191876 109521791 21319448 7279804 6800373\n",
      "14807165 89679326 5304644 33302396 3677132 32486295 46566310 8876414 55372519 6842067\n"
     ]
    }
   ],
   "source": [
    "# Now prepare a list of ten token ids randomly selected from fold08_1gram_agg\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "ten_random_token_ids = []\n",
    "\n",
    "for i in range(10):\n",
    "    ten_random_token_ids.append(random.sample(list(fold08_1gram_agg.keys()), 10))\n",
    "\n",
    "# List the ten random token ids horizontally\n",
    "\n",
    "for ten_random_token_ids in ten_random_token_ids:\n",
    "    print(*ten_random_token_ids, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untersuchungserfolg begreifenden Blechschirm 984193 034347/61 Tempogesellschaft Götz-Herrmann einenÜberraschungssieger Exklusivvertretung Notkunden\n",
      "8425 Klenower Tränenliste säkular-nationalen 2015GAnfang UngarischeMalerei 81-84 konnte.Höhepunkt Seung-hee eckert@wz.de\n",
      "Neutralitätsgedanke Österreicher-Vereinigung Religionskrise äVerhandeln Brausesackerln Kellergassen CeMAT-Präsidiums Nächtigungen.Im GreizEmma Brandschutzwochenende\n",
      "Barempfang hochspezifizierte Gewinner-Lösung Glühtee Smog-Tage Konzernsaal 21.45Tatort Ida-Helene HDMI-2.0-Buchsen Baecker-Handwerk\n",
      "Messröhrchen Saison-Klassierung artuntypisch Gerichtsübersetzer Kombi-Turniere Klassiktitel gerüstetGGreiz Daumennagel-großer Intensivstation-Patienten tarockierte\n",
      "Promi-Neugier Döhlauer Sek-Lehrer 110-kV-Oberleitung Country-Laune Musikerheuriger 2052,75 Fakultätsgutachten Kopf-Sprung Braten-Muffel\n",
      "Rosen-Schaubeete Jetzt-zeigen-wir Dröhn- Kanzlerinnen-Neuwahl Hornnagel Werkhausgasse Bauernkastln Levorato Grattagen abzuschließen.bre\n",
      "978-3-95441-510-6 Effektentasche Scheick-Verletzung Konservatoriumsangebot Bergamotte-Anbaus http://www.spiegel.de/politik/deutschland/arbeitsmigranten-regierung-will-staerker-nach-schwarzarbeitern-fahnden-a-759970.html wegene Frauenbund-Info Heinbichler 14.188\n",
      "Kapellenrenovierung Fahrrad-Juxrallye Marianne-Breuer-Verlags Bayern-Taktes Redaktions-Telefon Zangls nichtGTina FSJ-Teams beavip@cinestar.de Transaktionsbeschreibung\n",
      "Lopperbruch Saarbrücken2579 Zentralmessepalast Baiersbronn/23 Thalia-Harmonie ASTO-Sprecher konversationserfahren bereitstände BFC-Schülern wirksam.Haushalt\n"
     ]
    }
   ],
   "source": [
    "# In the above list, replace the token ids with actual tokens\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "ten_random_token_ids = []\n",
    "\n",
    "for i in range(10):\n",
    "    ten_random_token_ids.append(random.sample(list(fold08_1gram_agg.keys()), 10))\n",
    "\n",
    "for ten_random_token_ids in ten_random_token_ids:\n",
    "    print(*[token_keys08[token_id] for token_id in ten_random_token_ids], sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hatte lang er Kolsterbrunnen die diese Front defensiven und und\n",
      "ums bleiben Uhr machte will ( Trainer ergriffenen Hamburger der\n",
      "Kahlschlag Thorsten erledigt als nicht und . xyxHTMLyxy setzten Krulis\n",
      "Evangelischen Aber der Geburtstag Grasser Aber den ' und Croy\n",
      "Halbwahrheiten drückend Hilfe neuen der wird Forschungsbefunde Vielen für für\n",
      "Schüler , geprägt Damm losschlägt lassen die hielt Villmarer \"\n",
      "sollte erzählt stampfend Wartungs- Favoriten meinem Freundin vor Ideen '\n",
      "strotzt ihre sich , . Haus Projekt . 16.30 ,\n",
      ", . «END» gestern das Zeit Woche Weltmeister Parkplatzes hinter\n",
      "für muss zur , Mensch aus wie kosteten , bevor\n"
     ]
    }
   ],
   "source": [
    "# Now we repeat the above but this time, we draw tokens according to pm_sm_1gram\n",
    "\n",
    "def draw_random_tokens(p_1gram, total_tokens, unique_tokens, num_samples):\n",
    "    lambda_k = 1 / (total_tokens + unique_tokens)\n",
    "    weighted_probs = [(1-lambda_k) * p + lambda_k/unique_tokens for p in p_1gram.values()]\n",
    "    tokens = list(p_1gram.keys())\n",
    "    samples = random.choices(tokens, weights=weighted_probs, k=num_samples)\n",
    "    return samples\n",
    "\n",
    "num_samples = 10\n",
    "num_tokens = 10\n",
    "\n",
    "for i in range(num_samples):\n",
    "    random_tokens = draw_random_tokens(p_1gram, total_tokens, unique_tokens, num_tokens)\n",
    "    print(*[token_keys08[token_id] for token_id in random_tokens], sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100', '730', '30', '2438', '15', '662', '1467', '0']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's compute the probability of the sentence 'Wir arbeiten am Institut für Deutsche Sprache .'\n",
    "\n",
    "# We need a program that takes a sentence and returns a list of token ids based on the token_keys08 dictionary\n",
    "# If a token is not in the dictionary, we assign it the id -777\n",
    "\n",
    "def sentence_to_token_ids(sentence):\n",
    "    token_ids = []\n",
    "    for token in sentence.split():\n",
    "        try:\n",
    "            id = list(token_keys08.keys())[list(token_keys08.values()).index(token)]\n",
    "        except ValueError:\n",
    "            id = -777\n",
    "        token_ids.append(id)\n",
    "    return token_ids\n",
    "\n",
    "sentence_to_token_ids('Wir arbeiten am Institut für Deutsche Sprache .')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of sentence 'Wir arbeiten am Institut für Deutsche Sprache .': 2.2529093562602777e-26\n"
     ]
    }
   ],
   "source": [
    "# Now we use the sentence_to_token_ids function to compute the probability of the sentence 'Wir arbeiten am Institut für Deutsche Sprache .'\n",
    "\n",
    "def sentence_probability(sentence, p_1gram, total_tokens, unique_tokens):\n",
    "    token_ids = sentence_to_token_ids(sentence)\n",
    "    lambda_k = 1 / (total_tokens + unique_tokens)\n",
    "    prob = 1\n",
    "    for i in range(len(token_ids)):\n",
    "        w = token_ids[i]\n",
    "        if w in p_1gram:\n",
    "            p = (1-lambda_k) * p_1gram[w] + lambda_k/unique_tokens\n",
    "        else:\n",
    "            p = lambda_k/unique_tokens\n",
    "        prob *= p\n",
    "    return prob\n",
    "\n",
    "sentence = 'Wir arbeiten am Institut für Deutsche Sprache .'\n",
    "sentence_prob = sentence_probability(sentence, p_1gram, total_tokens, unique_tokens)\n",
    "print(f\"Probability of sentence '{sentence}': {sentence_prob}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-777]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try some gibberish\n",
    "\n",
    "# Find token ids for 'gibberish'\n",
    "\n",
    "sentence_to_token_ids('gibberish')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of sentence 'gibberish': 1.8601846398709812e-17\n"
     ]
    }
   ],
   "source": [
    "# Since this is not available in the dictionary, we assign it the id -777.\n",
    "# What is the smoothed probability of the sentence 'gibberish'\n",
    "\n",
    "sentence = 'gibberish'\n",
    "sentence_prob = sentence_probability(sentence, p_1gram, total_tokens, unique_tokens)\n",
    "print(f\"Probability of sentence '{sentence}': {sentence_prob}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to\n",
    "$ \\frac{1}{N+K} \\frac{1}{K}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8601846398709812e-17\n"
     ]
    }
   ],
   "source": [
    "print(1/(total_tokens + unique_tokens)*1/unique_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of sentence 'Das ist ein richtiger Satz .': 1.18e-18\n"
     ]
    }
   ],
   "source": [
    "# Let's compute the probability of the sentence 'Das ist ein richtiger Satz .'\n",
    "\n",
    "sent1 = 'Das ist ein richtiger Satz .'\n",
    "sent1_prob = sentence_probability(sent1, p_1gram, total_tokens, unique_tokens)\n",
    "print(f\"Probability of sentence '{sent1}': {sent1_prob:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of sentence 'Das ist ein richtige Satz .': 1.16e-17\n"
     ]
    }
   ],
   "source": [
    "# What about an ungrammatical sentence, e.g. 'Das ist ein richtige Satz .'\n",
    "\n",
    "sent2 = 'Das ist ein richtige Satz .'\n",
    "sent2_prob = sentence_probability(sent2, p_1gram, total_tokens, unique_tokens)\n",
    "print(f\"Probability of sentence '{sent2}': {sent2_prob:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Das ist ein richtige Satz .\" is 9.85 times more probable than \"Das ist ein richtiger Satz .\"\n"
     ]
    }
   ],
   "source": [
    "# Compute ratio of probabilities\n",
    "\n",
    "ratio = sent2_prob / sent1_prob\n",
    "\n",
    "# Print the ratio\n",
    "\n",
    "print('\"Das ist ein richtige Satz .\" is {:.2f} times more probable than \"Das ist ein richtiger Satz .\"'.format(ratio))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Das ist ein richtige Satz .\" is 9.85 times more probable than \"Das ist ein richtiger Satz .\"\n",
    "This is because our model only is a so called 'bag-of-words' model, i.e. it does not take into account the order of words. 'richtiger' is simply more frequent than 'richtige' in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of \"richtiger\": 0.00001\n",
      "Probability of \"richtige\": 0.00005\n"
     ]
    }
   ],
   "source": [
    "# Compute the probability of 'richtiger' and 'richtige'\n",
    "\n",
    "p_richtiger = sentence_probability('richtiger', p_1gram, total_tokens, unique_tokens)\n",
    "p_richtige = sentence_probability('richtige', p_1gram, total_tokens, unique_tokens)\n",
    "\n",
    "# Print\n",
    "\n",
    "print(f'Probability of \"richtiger\": {p_richtiger:.5f}')\n",
    "print(f'Probability of \"richtige\": {p_richtige:.5f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9.85'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{:.2f}'.format(p_richtige/p_richtiger)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we should be able to improve our model by incorporating the context of the word. Let's train a smoothed 2-gram model.\n",
    "The smoothed conditional probability of w given v can be calculated as:\n",
    "\n",
    "$P_{sm} (w│v)=λ_v P_{ml} (w│v)+(1-λ_v)P_{sm} (w)$\n",
    "\n",
    "where $P_{ml} (w│v)$ is the maximum likelihood estimate of the probability of word $w$ given word $v$ and where $λ_v=\\frac{C(v)}{C(v)+γ_v }$ is calculated based on $γ_v$ that denotes the number of different words observed after $v$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the 2-gram frequencies for fold 08 consisting of ```token id1, lemma id1, pos1, token id2, lemma id2, pos2, frequency```\n",
    "\n",
    "We only need ```token id1, token id2, and frequency```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing aggregated 2-gram frequencies in fold08_2gram_agg.pkl\n"
     ]
    }
   ],
   "source": [
    "fold08_2gram_agg = defaultdict(int)\n",
    "\n",
    "with open('2-gram-token-lemma-pos-freqs-with-punctuation.08.tsv', 'r') as freq_file:\n",
    "    freq_reader = csv.reader(freq_file, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in freq_reader:\n",
    "        token_id1, lemma_id1, pos1, token_id2, lemma_id2, pos2, frequency = row\n",
    "        # Aggregate the frequency by token_id1 and token_id2\n",
    "        fold08_2gram_agg[(token_id1, token_id2)] += int(frequency)\n",
    "\n",
    "# Store the aggregated 2-gram frequencies in a pickle file\n",
    "print('Storing aggregated 2-gram frequencies in fold08_2gram_agg.pkl')\n",
    "with open('fold08_2gram_agg.pkl', 'wb') as f:\n",
    "    pickle.dump(fold08_2gram_agg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('0', '20'), 10394998),\n",
       " (('0', '5'), 9403062),\n",
       " (('-2', '-2'), 9107988),\n",
       " (('-1', '-1'), 9107988),\n",
       " (('6', '2'), 8437829),\n",
       " (('1', '39'), 6867551),\n",
       " (('1', '3'), 9999401),\n",
       " (('5', '1'), 6305548),\n",
       " (('0', '33'), 5784250),\n",
       " (('0', '-2'), 4496264)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the 10 first entries of fold08_2gram_agg\n",
    "\n",
    "list(fold08_2gram_agg.items())[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $P_{ml} =\\frac{c(v,w)}{c(v)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing 2-gram probabilities in p_2gram.pkl\n"
     ]
    }
   ],
   "source": [
    "def compute_conditional_probs(fold08_2gram_agg, fold08_1gram_agg):\n",
    "    p_2gram = {}\n",
    "    for (v, w), freq_wv in fold08_2gram_agg.items():\n",
    "        freq_v = fold08_1gram_agg[v]\n",
    "        p_v_given_w = freq_wv / freq_v\n",
    "        if v not in p_2gram:\n",
    "            p_2gram[v] = {}\n",
    "        p_2gram[v][w] = p_v_given_w\n",
    "    \n",
    "    for v in p_2gram.keys():\n",
    "        total = sum(p_2gram[v].values())\n",
    "        for w in p_2gram[v].keys():\n",
    "            p_2gram[v][w] /= total\n",
    "    \n",
    "    return p_2gram\n",
    "\n",
    "p_2gram = compute_conditional_probs(fold08_2gram_agg, fold08_1gram_agg)\n",
    "\n",
    "# Store the 2-gram probabilities in a pickle file\n",
    "print('Storing 2-gram probabilities in p_2gram.pkl')\n",
    "with open('p_2gram.pkl', 'wb') as f:\n",
    "    pickle.dump(p_2gram, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to compute weights $λ_v=\\frac{C(v)}{C(v)+γ_v }$ that are calculated based on $γ_v$ that denotes the number of different words observed after $v$.  We can do this by using the 'fold08_2gram_agg' dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing weights in lambdas_2gram.pkl\n"
     ]
    }
   ],
   "source": [
    "def compute_weights(fold08_2gram_agg, fold08_1gram_agg):\n",
    "    weights = {}\n",
    "    for (v, w), freq_wv in fold08_2gram_agg.items():\n",
    "        if v not in weights:\n",
    "            weights[v] = 0\n",
    "        weights[v] += 1\n",
    "        \n",
    "    for v, num_w in weights.items():\n",
    "        freq_v = fold08_1gram_agg[v]\n",
    "        weights[v] = freq_v / (freq_v + num_w)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "lambdas_2gram = compute_weights(fold08_2gram_agg, fold08_1gram_agg)\n",
    "\n",
    "# Store the weights in a pickle file\n",
    "print('Storing weights in lambdas_2gram.pkl')\n",
    "with open('lambdas_2gram.pkl', 'wb') as f:\n",
    "    pickle.dump(lambdas_2gram, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', 0.9877858773490689),\n",
       " ('-2', 0.9999998902062793),\n",
       " ('-1', 0.9168784375436645),\n",
       " ('6', 0.9875822418016662),\n",
       " ('1', 0.9870102987693855),\n",
       " ('5', 0.9648158064744667),\n",
       " ('15', 0.9718534878399419),\n",
       " ('19', 0.9790236724640878),\n",
       " ('34', 0.9874086259889072),\n",
       " ('8', 0.9617792749746221)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the 10 first entries of lambdas_2gram\n",
    "\n",
    "list(lambdas_2gram.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'67932': 0.20000000000000004,\n",
       " '3209': 0.16666666666666669,\n",
       " '514': 0.13333333333333336,\n",
       " '20061': 0.06666666666666668,\n",
       " '30860': 0.06666666666666668,\n",
       " '1503': 0.03333333333333334,\n",
       " '131807': 0.03333333333333334,\n",
       " '4434': 0.03333333333333334,\n",
       " '2974': 0.03333333333333334,\n",
       " '181154': 0.03333333333333334,\n",
       " '930477': 0.03333333333333334,\n",
       " '2198576': 0.03333333333333334,\n",
       " '49500': 0.03333333333333334,\n",
       " '38939': 0.03333333333333334,\n",
       " '5951': 0.03333333333333334,\n",
       " '3087': 0.03333333333333334}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List entries of p_2gram for 'weinerlichem'\n",
    "\n",
    "token = 'weinerlichem'\n",
    "token_id = list(token_keys08.keys())[list(token_keys08.values()).index(token)]\n",
    "p_2gram[token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check count of 'weinerlichem'\n",
    "\n",
    "fold08_1gram_agg[token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6521739130434783"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With that, we should be able to compute the lambda of 'weinerlichem'\n",
    "\n",
    "# 16 different tokens follow 'weinerlichem' in the corpus\n",
    "\n",
    "len(p_2gram[token_id])\n",
    "\n",
    "# Compute lambda\n",
    "\n",
    "fold08_1gram_agg[token_id] / (fold08_1gram_agg[token_id] + len(p_2gram[token_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the compute weight of 'weinerlichem'\n",
    "\n",
    "lambdas_2gram[token_id]\n",
    "\n",
    "assert lambdas_2gram[token_id] == fold08_1gram_agg[token_id] / (fold08_1gram_agg[token_id] + len(p_2gram[token_id]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we we can draw tokens according to 'pm_sm_2gram' (note that we still use 'pm_sm_1gram' to draw the first token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing smoothed 1-gram probabilities in p_1gram_smoothed.pkl\n"
     ]
    }
   ],
   "source": [
    "# First we pre-compute smoothed probabilities for the 1-gram model and store them in a dictionary\n",
    "\n",
    "p_1gram_smoothed = {}\n",
    "\n",
    "for token_id, freq in fold08_1gram_agg.items():\n",
    "    p_1gram_smoothed[token_id] = (freq + 1) / (total_tokens + unique_tokens)\n",
    "\n",
    "# Store the smoothed 1-gram probabilities in a pickle file\n",
    "\n",
    "print('Storing smoothed 1-gram probabilities in p_1gram_smoothed.pkl')\n",
    "\n",
    "with open('p_1gram_smoothed.pkl', 'wb') as f:\n",
    "    pickle.dump(p_1gram_smoothed, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_random_tokens(p_1gram_smoothed, p_2gram, lambdas_2gram, num_tokens=10):\n",
    "    tokens = []\n",
    "    \n",
    "    # Draw the first token from the 1-gram model\n",
    "    first_token = random.choices(list(p_1gram_smoothed.keys()), weights=list(p_1gram_smoothed.values()), k=1)[0]\n",
    "    tokens.append(first_token)\n",
    "    \n",
    "    # Draw the remaining tokens from the 2-gram model\n",
    "    for i in range(1, num_tokens):\n",
    "        last_token = tokens[-1]\n",
    "        if last_token in p_2gram:\n",
    "            cond_probs = {token: lambdas_2gram[last_token] * p_2gram[last_token][token] + (1 - lambdas_2gram[last_token]) * p_1gram_smoothed[token] for token in p_2gram[last_token]}\n",
    "            next_token = random.choices(list(cond_probs.keys()), weights=list(cond_probs.values()), k=1)[0]\n",
    "            tokens.append(next_token)\n",
    "        else:\n",
    "            tokens.append(random.choices(list(p_1gram_smoothed.keys()), weights=list(p_1gram_smoothed.values()), k=1)[0])\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helmut Kohl musste um die Lage auf die bei einer Agrarwende , Kraft erfordern . Alle kamen die von 2,05\n",
      "ist nicht unbedingt verschwinden lasse sich die keine Randerscheinungen Marmagener CDU-Landtagsabgeordnete Reinhold Gebhardt Wolfgang Steinmetz am Mittwoch , die 2010\n",
      ", das Land ? Gino Pace , wurde 1990 , die Schützenkönigin war aussichtslos : Kein Wunder , heute gegen\n",
      "zu der Natur- und wie sie wollen wir drüber fuhren die Leiter des Lebens ( ‘ Es war . Das\n",
      "des als Stürmer auf Wiedergewinnung alter Wunsch aus - Zeitsoldat bei WM-Endrunden . Wochenlang tot . Und dann ein erstklassiges\n",
      "TV-Foto : „ Königshaus . Ob Omas und später auch Pokalfinalist Eintracht am häufigsten Erreger in Pentz startet die in\n",
      "zum Wechsel im Rahmen des Beförderungsbudgets für die Liste / Bratislava ) sowie altersgerechtes Vereinstraining dienstags 13 Uhr , Gott\n",
      "\" Er war geschlagen worden . Viele Leistungsträger in der Furcht vor allem an . Freispruch , fuhren abends die\n",
      "starken Offensive gegen die mutigen Entschluss zum Saisonausklang Mountainbike : \" auslaufen . Man Conference . Beim mit Heiner Schepp\n",
      "ockergelber Acrylfarbe erreichen . Die Tagesordnung der Rezepte an Remo haben die Aufmerksamkeit verschiedener Parts an Bord . Sybille Schilling\n"
     ]
    }
   ],
   "source": [
    "num_samples = 10\n",
    "num_tokens = 20\n",
    "\n",
    "for i in range(num_samples):\n",
    "    random_tokens = draw_random_tokens(p_1gram_smoothed, p_2gram, lambdas_2gram, num_tokens)\n",
    "    print(*[token_keys08[token_id] for token_id in random_tokens], sep=' ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the probality of the sentences '\"Das ein ist richtige Satz .\" and \"Das ist ein richtiger Satz .\" using our smoothed 2-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sentence_to_token_ids() to convert the sentences to token IDs\n",
    "\n",
    "# Define a program 'compute_probability_of_sentence' that computes the probability of a sentence\n",
    "\n",
    "def compute_probability_of_sentence(sentence, p_1gram_smoothed, p_2gram, lambdas_2gram):\n",
    "    tokens = sentence_to_token_ids(sentence)\n",
    "    prob = 1\n",
    "    # Draw the first token from the 1-gram model\n",
    "    first_token = tokens[0]\n",
    "    prob *= p_1gram_smoothed[first_token]\n",
    "    # Draw the remaining tokens from the 2-gram model\n",
    "    for i in range(1, len(tokens)):\n",
    "        last_token = tokens[i - 1]\n",
    "        token = tokens[i]\n",
    "        if last_token in p_2gram and token in p_2gram[last_token]:\n",
    "            prob *= lambdas_2gram[last_token] * p_2gram[last_token][token] + (1 - lambdas_2gram[last_token]) * p_1gram_smoothed[token]\n",
    "        else:\n",
    "            prob *= p_1gram_smoothed[token]\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of \"Das ist ein richtige Satz .\" is 8.13e-17\n"
     ]
    }
   ],
   "source": [
    "sent1 = compute_probability_of_sentence('Das ist ein richtige Satz .', p_1gram_smoothed, p_2gram, lambdas_2gram)\n",
    "print('The probability of \"Das ist ein richtige Satz .\" is {:.2e}'.format(sent1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of \"Das ist ein richtiger Satz .\" is 2.22e-13\n"
     ]
    }
   ],
   "source": [
    "sent2 = compute_probability_of_sentence('Das ist ein richtiger Satz .', p_1gram_smoothed, p_2gram, lambdas_2gram)\n",
    "print('The probability of \"Das ist ein richtiger Satz .\" is {:.2e}'.format(sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Das ist ein richtiger Satz .\" is around 2724 times more probable than \"Das ist ein richtige Satz .\"\n"
     ]
    }
   ],
   "source": [
    "print('\"Das ist ein richtiger Satz .\" is around {:.0f} times more probable than \"Das ist ein richtige Satz .\"'.format(sent2/sent1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the conditional probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(richtige|ein) 3.19724849917519e-06\n",
      "p(Satz|richtige) 6.403467638530913e-05\n",
      "p(richtiger|ein) 0.0004839062838288913\n",
      "p(Satz|richtiger) 0.001152413788250634\n"
     ]
    }
   ],
   "source": [
    "print ('p(richtige|ein)',lambdas_2gram[sentence_to_token_ids('ein')[0]] * p_2gram[sentence_to_token_ids('ein')[0]][sentence_to_token_ids('richtige')[0]] + (1 - lambdas_2gram[sentence_to_token_ids('ein')[0]]) * p_1gram_smoothed[sentence_to_token_ids('richtige')[0]])\n",
    "print ('p(Satz|richtige)',lambdas_2gram[sentence_to_token_ids('richtige')[0]] * p_2gram[sentence_to_token_ids('richtige')[0]][sentence_to_token_ids('Satz')[0]] + (1 - lambdas_2gram[sentence_to_token_ids('richtige')[0]]) * p_1gram_smoothed[sentence_to_token_ids('Satz')[0]])\n",
    "\n",
    "\n",
    "print ('p(richtiger|ein)',lambdas_2gram[sentence_to_token_ids('ein')[0]] * p_2gram[sentence_to_token_ids('ein')[0]][sentence_to_token_ids('richtiger')[0]] + (1 - lambdas_2gram[sentence_to_token_ids('ein')[0]]) * p_1gram_smoothed[sentence_to_token_ids('richtiger')[0]])\n",
    "print ('p(Satz|richtiger)',lambdas_2gram[sentence_to_token_ids('richtiger')[0]] * p_2gram[sentence_to_token_ids('richtiger')[0]][sentence_to_token_ids('Satz')[0]] + (1 - lambdas_2gram[sentence_to_token_ids('richtiger')[0]]) * p_1gram_smoothed[sentence_to_token_ids('Satz')[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue with a 3-gram model, by computing\n",
    "\n",
    "$P_{sm} (w│u,v)=\\frac{c(uvw)+γ_{u,v} P_{sm} (w│u,v)}{C(u,v)+γ_{u,v}}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the 3-gram frequencies for fold 08 consisting of ````token id1, lemma id1, pos1, token id2, lemma id2, pos2, token id3, lemma id3, pos3, frequency````\n",
    "\n",
    "We only need ````token id1, token id2, token id3, and frequency````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing aggregated 3-gram frequencies in fold08_3gram_agg.pkl\n"
     ]
    }
   ],
   "source": [
    "fold08_3gram_agg = defaultdict(int)\n",
    "\n",
    "with open('3-gram-token-lemma-pos-freqs-with-punctuation.08.tsv', 'r') as freq_file:\n",
    "    freq_reader = csv.reader(freq_file, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in freq_reader:\n",
    "        token_id1, lemma_id1, pos1, token_id2, lemma_id2, pos2, token_id3, lemma_id3, pos3, frequency = row\n",
    "        # Here, rows where token_id1, token_id2 and token_id3 are -1 or -2  can be removed\n",
    "        if not (token_id1 in ['-1', '-2'] and token_id2 in ['-1', '-2'] and token_id3 in ['-1', '-2']):\n",
    "            # Only append token_id1, token_id2, token_id3, and frequency\n",
    "            fold08_3gram_agg[(token_id1, token_id2, token_id3)] += int(frequency)\n",
    "\n",
    "# Store the aggregated 3-gram frequencies in a pickle file\n",
    "print('Storing aggregated 3-gram frequencies in fold08_3gram_agg.pkl')\n",
    "with open('fold08_3gram_agg.pkl', 'wb') as f:\n",
    "    pickle.dump(fold08_3gram_agg, f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P_{sm} (w│u,v)=λ_v P_{ml} (w│u,v)+(1-λ_v)P_{sm} (w|v)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing 3-gram probabilities in p_3gram.pkl\n"
     ]
    }
   ],
   "source": [
    "def compute_conditional_probs(fold08_3gram_agg, fold08_2gram_agg):\n",
    "    p_3gram = {}\n",
    "\n",
    "    for (u, v, w), freq_uwv in fold08_3gram_agg.items():\n",
    "        freq_uv = fold08_2gram_agg[(u, v)]\n",
    "        p_w_given_uv = freq_uwv / freq_uv\n",
    "        if (u, v) not in p_3gram:\n",
    "            p_3gram[(u, v)] = {}\n",
    "        p_3gram[(u, v)][w] = p_w_given_uv\n",
    "\n",
    "    for (u, v), p_w_given_uv in p_3gram.items():\n",
    "        total = sum(p_3gram[(u, v)].values())\n",
    "        for w in p_3gram[(u, v)]:\n",
    "            p_3gram[(u, v)][w] /= total\n",
    "\n",
    "    return p_3gram\n",
    "\n",
    "p_3gram = compute_conditional_probs(fold08_3gram_agg, fold08_2gram_agg)\n",
    "\n",
    "# Store the 3-gram probabilities in a pickle file\n",
    "print('Storing 3-gram probabilities in p_3gram.pkl')\n",
    "with open('p_3gram.pkl', 'wb') as f:\n",
    "    pickle.dump(p_3gram, f)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing weights in lambdas_3gram.pkl\n"
     ]
    }
   ],
   "source": [
    "def compute_weights(fold08_3gram_agg, fold08_2gram_agg, fold08_1gram_agg):\n",
    "    weights = {}\n",
    "    for (v_1, v_2, w), freq_wv in fold08_3gram_agg.items():\n",
    "        if (v_1, v_2) not in weights:\n",
    "            weights[(v_1, v_2)] = 0\n",
    "        weights[(v_1, v_2)] += 1\n",
    "\n",
    "    for (v_1, v_2), num_w in weights.items():\n",
    "        freq_v = fold08_2gram_agg[(v_1, v_2)]\n",
    "        weights[(v_1, v_2)] = freq_v / (freq_v + num_w)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "lambdas_3gram = compute_weights(fold08_3gram_agg, fold08_2gram_agg, fold08_1gram_agg)\n",
    "\n",
    "# Store the weights in a pickle file\n",
    "print('Storing weights in lambdas_3gram.pkl')\n",
    "with open('lambdas_3gram.pkl', 'wb') as f:\n",
    "    pickle.dump(lambdas_3gram, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_random_tokens(p_1gram_smoothed, p_2gram, p_3gram, lambdas_2gram, lambdas_3gram, num_tokens=10):\n",
    "    tokens = []\n",
    "    \n",
    "    # Draw the first token from the smoothed unigram probability distribution\n",
    "    first_token = random.choices(list(p_1gram_smoothed.keys()), weights=list(p_1gram_smoothed.values()), k=1)[0]\n",
    "    tokens.append(first_token)\n",
    "    \n",
    "    # Draw the second token from the smoothed bigram probability distribution\n",
    "    last_token = tokens[-1]\n",
    "    cond_probs = {token: lambdas_2gram[last_token] * p_2gram[last_token][token] + (1 - lambdas_2gram[last_token]) * p_1gram_smoothed[token] for token in p_2gram[last_token]}\n",
    "    next_token = random.choices(list(cond_probs.keys()), weights=list(cond_probs.values()), k=1)[0]\n",
    "    tokens.append(next_token)\n",
    "    \n",
    "    # Draw the remaining tokens from the smoothed trigram probability distribution\n",
    "    for i in range(2, num_tokens):\n",
    "        last_two_tokens = (tokens[-2], tokens[-1])\n",
    "        if last_two_tokens in p_3gram:\n",
    "            cond_probs = {token: lambdas_3gram[last_two_tokens] * p_3gram[last_two_tokens][token] + (1 - lambdas_3gram[last_two_tokens]) * p_2gram[last_two_tokens[1]][token] for token in p_3gram[last_two_tokens]}\n",
    "            next_token = random.choices(list(cond_probs.keys()), weights=list(cond_probs.values()), k=1)[0]\n",
    "            tokens.append(next_token)\n",
    "        else:\n",
    "            cond_probs = {token: lambdas_2gram[last_token] * p_2gram[last_token][token] + (1 - lambdas_2gram[last_token]) * p_1gram_smoothed[token] for token in p_2gram[last_token]}\n",
    "            next_token = random.choices(list(cond_probs.keys()), weights=list(cond_probs.values()), k=1)[0]\n",
    "            tokens.append(next_token)\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gesicht der einheimischen Bürger , bei dem Text \" Auf die Juristen in der L-Dressur den Sieg des bereits sehr\n",
      "Geöffnet Die Schlossweihnacht hat Mo-Fr von 15 bis 17.15 Uhr ) steigt das Risiko fu r Humor , Seifenblasen-Wölkchen statt\n",
      "erfolgreichen gemeinsamen Bemühungen um das BIP sank von 18,4 Prozent liegt der nächste Premierminister wird . Die Verträge für ein\n",
      "zu vertreiben . Vergeblich mühten sich Nachwuchsfußballer aus der Lüftungsanlage der Parkgarage bezeichnen . Klar ist , mit 6 :\n",
      "an Flüssigkeit und eine kleinere Rolle . Mit einem schweren Unfall ein \" complete pictoral version \" ist es vorgekommen\n",
      "ein 0 : 1. Anne Rudolf vom Weingrosshändler Schlumberger im RP Shop für Reitsportfans der näheren Zukunft zugunsten des Senders\n",
      "während seiner Amtszeit ( Juni ) , an die Vergangenheit Günter Kommol zeigte Ehemaligen aus den diesjährigen Wettbewerb organisiert .\n",
      ", entfalte sich die Verwaltung kaum dokumentieren . Doch der TAZ beisteuern ( 2015 ) nahm , gilt als wichtiger\n",
      "Leute beziehen sich mal mit Weitschüssen , beim Allianzparkplatz , übermorgen \" ausgerichtet sei , erklärte der alte Hausmeister der\n",
      "Bericht der britischen Fachzeitschrift \" Scientific Community Gastkommentar G Gastkommentar . Trotz guter Personalausstattung der Kirchenspitze unzufrieden , da es\n"
     ]
    }
   ],
   "source": [
    "num_samples = 10\n",
    "num_tokens = 20\n",
    "\n",
    "for i in range(num_samples):\n",
    "    random_tokens = draw_random_tokens(p_1gram_smoothed, p_2gram, p_3gram, lambdas_2gram, lambdas_3gram, num_tokens=num_tokens)\n",
    "    print(*[token_keys08[token_id] for token_id in random_tokens], sep=' ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again let's compute the probality of the sentences '\"Das ein ist richtige Satz .\" and \"Das ist ein richtiger Satz .\". Now we use our smoothed 3-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probability_of_sentence(sentence, p_1gram_smoothed, p_2gram, p_3gram, lambdas_2gram, lambdas_3gram):\n",
    "    tokens = sentence_to_token_ids(sentence)\n",
    "    prob = 1\n",
    "    # Draw the first token from the 1-gram model\n",
    "    first_token = tokens[0]\n",
    "    prob *= p_1gram_smoothed[first_token]\n",
    "    \n",
    "    # Draw the second token from the 2-gram model\n",
    "    if len(tokens) > 1:\n",
    "        second_token = tokens[1]\n",
    "        last_token = tokens[0]\n",
    "        if last_token in p_2gram and second_token in p_2gram[last_token]:\n",
    "            prob *= lambdas_2gram[last_token] * p_2gram[last_token][second_token] + (1 - lambdas_2gram[last_token]) * p_1gram_smoothed[second_token]\n",
    "        else:\n",
    "            prob *= p_1gram_smoothed[second_token]\n",
    "    \n",
    "    # Draw the remaining tokens from the 3-gram model\n",
    "    for i in range(2, len(tokens)):\n",
    "        last_2_tokens = (tokens[i-2], tokens[i-1])\n",
    "        token = tokens[i]\n",
    "        if last_2_tokens in p_3gram and token in p_3gram[last_2_tokens]:\n",
    "            prob *= lambdas_3gram[last_2_tokens] * p_3gram[last_2_tokens][token] + (1 - lambdas_3gram[last_2_tokens]) * p_2gram[last_2_tokens[1]][token]\n",
    "        else:\n",
    "            prob *= p_2gram[last_2_tokens[1]][token] if last_2_tokens[1] in p_2gram and token in p_2gram[last_2_tokens[1]] else p_1gram_smoothed[token]\n",
    "    \n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of \"Das ist ein richtiger Satz .\" is 7.45e-12\n"
     ]
    }
   ],
   "source": [
    "sent1 = compute_probability_of_sentence('Das ist ein richtiger Satz .', p_1gram_smoothed, p_2gram, p_3gram, lambdas_2gram, lambdas_3gram)\n",
    "print('The probability of \"Das ist ein richtiger Satz .\" is {:.2e}'.format(sent1))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of \"Das ist ein richtige Satz .\" is 1.23e-15\n"
     ]
    }
   ],
   "source": [
    "sent2 = compute_probability_of_sentence('Das ist ein richtige Satz .', p_1gram_smoothed, p_2gram, p_3gram, lambdas_2gram, lambdas_3gram)\n",
    "print('The probability of \"Das ist ein richtige Satz .\" is {:.2e}'.format(sent2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there's a 3-gram \"ein\", \"richtiger\" and \"Satz\" in p_3gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0012951503813498188"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'ein richtiger Satz'\n",
    "tokens = sentence_to_token_ids(sentence)\n",
    "\n",
    "# Check if there's a 3-gram \"ein\", \"richtige\" and \"Satz\" in p_3gram\n",
    "last_two_tokens = (tokens[0], tokens[1])\n",
    "next_token = tokens[2]\n",
    "\n",
    "p_3gram[last_two_tokens][next_token] if last_two_tokens in p_3gram and next_token in p_3gram[last_two_tokens] else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there's a 3-gram \"ein\", \"richtige\" and \"Satz\" in p_3gram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'ein richtige Satz'\n",
    "tokens = sentence_to_token_ids(sentence)\n",
    "\n",
    "# Check if there's a 3-gram \"ein\", \"richtige\" and \"Satz\" in p_3gram\n",
    "last_two_tokens = (tokens[0], tokens[1])\n",
    "next_token = tokens[2]\n",
    "\n",
    "p_3gram[last_two_tokens][next_token] if last_two_tokens in p_3gram and next_token in p_3gram[last_two_tokens] else 0\t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the conditional probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(Satz|ein, richtige) 6.403467638530913e-05\n"
     ]
    }
   ],
   "source": [
    "last_two_tokens = (sentence_to_token_ids('ein')[0], sentence_to_token_ids('richtige')[0])\n",
    "next_token = sentence_to_token_ids('Satz')[0]\n",
    "\n",
    "if last_two_tokens in p_3gram and next_token in p_3gram[last_two_tokens]:\n",
    "    prob = lambdas_3gram[last_two_tokens] * p_3gram[last_two_tokens][next_token] + (1 - lambdas_3gram[last_two_tokens]) * (lambdas_2gram[last_two_tokens[-1]] * p_2gram[last_two_tokens[-1]][next_token] + (1 - lambdas_2gram[last_two_tokens[-1]]) * p_1gram_smoothed[next_token])\n",
    "else:\n",
    "    prob = lambdas_2gram[last_two_tokens[-1]] * p_2gram[last_two_tokens[-1]][next_token] + (1 - lambdas_2gram[last_two_tokens[-1]]) * p_1gram_smoothed[next_token]\n",
    "\n",
    "print ('p(Satz|ein, richtige)', prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(Satz|ein, richtiger) 0.0012552842078156863\n"
     ]
    }
   ],
   "source": [
    "last_two_tokens = (sentence_to_token_ids('ein')[0], sentence_to_token_ids('richtiger')[0])\n",
    "next_token = sentence_to_token_ids('Satz')[0]\n",
    "\n",
    "if last_two_tokens in p_3gram and next_token in p_3gram[last_two_tokens]:\n",
    "    prob = lambdas_3gram[last_two_tokens] * p_3gram[last_two_tokens][next_token] + (1 - lambdas_3gram[last_two_tokens]) * (lambdas_2gram[last_two_tokens[-1]] * p_2gram[last_two_tokens[-1]][next_token] + (1 - lambdas_2gram[last_two_tokens[-1]]) * p_1gram_smoothed[next_token])\n",
    "else:\n",
    "    prob = lambdas_2gram[last_two_tokens[-1]] * p_2gram[last_two_tokens[-1]][next_token] + (1 - lambdas_2gram[last_two_tokens[-1]]) * p_1gram_smoothed[next_token]\n",
    "\n",
    "print ('p(Satz|ein, richtiger)', prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] Wolfer, Sascha, Koplenig, Alexander, Kupietz, Marc & Müller-Spitzer, Carolin. submitted. Introducing DeReKoGram.\n",
    "\n",
    "[2] Brants, Thorsten & Popat, Ashok C. & Xu, Peng & Och, Franz J. & Dean, Jeffrey. 2007. Large language models in machine translation. In Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning (EMNLP-CoNLL), 858–867. Prague, Czech Republic: Association for Computational Linguistics. Retrieved from [https://aclanthology.org/D07-1090](https://aclanthology.org/D07-1090)\n",
    "\n",
    "[3] Schmid, Helmut. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. In International Conference on New Methods in Language Processing,. Manchester, UK. Retrieved from [https://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/data/tree-tagger1](https://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/data/tree-tagger1.pdf)\n",
    "\n",
    "[4] Wolfer, Sascha & Hein, Katrin. 2022. Konsequenzen der los-Suffigierung im Deutschen: Korpushäufigkeit, emotional-affektive Effekte und konstruktionsgrammatische Perspektiven. Zeitschrift für Wortbildung / Journal of Word Formation 6(2). 71–99. DOI: [https://doi.org/10.3726/zwjw.2022.02.03](https://doi.org/10.3726/zwjw.2022.02.03)  \n",
    "\n",
    "[5] Stadler, Heike. 2014. Die Erstellung der Basislemmaliste der neuhochdeutschen Standardsprache aus mehrfach linguistisch annotierten Korpora. (H. Blühdorn & M. Elstermann & A. Klosa, Eds.). Mannheim: Institut für Deutsche Sprache. Retrieved from [https://nbn-resolving.org/urn:nbn:de:bsz:mh39-29999](https://nbn-resolving.org/urn:nbn:de:bsz:mh39-29999)\n",
    "\n",
    "[6] Chen, Stanley F. & Joshua Goodman. 1996. An Empirical Study of Smoothing Techniques for Language Modeling. In 34th Annual Meeting of the Association for Computational Linguistics, 310–318. Santa Cruz, California, USA: Association for Computational Linguistics. https://doi.org/10.3115/981863.981904. [https://aclanthology.org/P96-1041](https://aclanthology.org/P96-1041).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer: I used `GitHub Copilot` and `ChatGPT` when producing this notebook.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
